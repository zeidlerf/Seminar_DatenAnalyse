{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beispiel: Analyse eines Datensatzes\n",
    "\n",
    "Anhand eines beispielhaften Datensatz soll im Folgenden eine explorative Datenanalyse in Python erläutert werden. \n",
    "\n",
    "Als Quelle dient ein Datensatz der Datenanalyse Plattform [Kaggle](www.kaggle.com). Auf dieser kostenlosen Webseite werden Datensätze bereitsgestellt, die von Nutzern in Python oder R analysiert und dann vorgestellt werden. Die Plattform ist also auch eine gute Anlaufstelle zum Erlernen von Datenanalysefähigkeiten. \n",
    "\n",
    "Der Datensatz besteht aus 537.577 Zeilen und 12 Spalten. Jede der `537.577` Zeilen ist eine Transaktion eines Kunden bei einer Handelskette am sogenannten **Black Friday**. Für jede Transaktion sind insgesamt 12 sogenannte Merkmale (die Spalten) festgehalten. \n",
    "\n",
    "`USER_ID` = ID des Käufers\n",
    "\n",
    "`Produkt_ID` = ID des Produktes, welches gekauft wurde\n",
    "\n",
    "`Gender` = Geschlecht des Käufers\n",
    "\n",
    "`Age` = Alter des Käufers (in Alterskategorien)\n",
    "\n",
    "`Occupation` = Beruf des Käufers\n",
    "\n",
    "`City_Category` = Stadtkategorie, aus der der Käufer stammt (bzw. die Transaktion getätigt hat)\n",
    "\n",
    "`Stay_In_Current_City_Year` = wie lange ist User bereits in der jeweiligen Stadt\n",
    "\n",
    "`Marital_Status` = Ist der User verheiratet oder nicht\n",
    "\n",
    "`Product_Category_1` = Kategorie des Produktes (Kategoriegruppe 1)\n",
    "\n",
    "`Product_Category_2` = Kategorie des Produktes (Kategoriegruppe 2)\n",
    "\n",
    "`Product_Category_3` = Kategorie des Produktes (Kategoriegruppe 3)\n",
    "\n",
    "`Purchase` = Umsatz der Transaktion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importieren der relevanten Bibliotheken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zu Beginn einer Analyse sollten zunächst die relevanten (d.h. die benötigten) Bibliotheken importiert werden. Auch wenn im Verlaufe der Analyse weitere Bibliotheken hinzukommen, sollten diese hier hinzugefügt werden. So ist sichergestellt, dass jeder Nutzer des Notebooks sofort nachvollziehen kann, welche Module zum Einsatz kommen und wie diese importiert wurden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "%matplotlib inline \n",
    "plt.style.use(\"seaborn-notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einlesen der Rohdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die nötigen Bibliotheken importiert wurden, lesen wir den zu analysierenden Datensatz ein. In Realität kann es sich an dieser Stelle bereits um viele Datensätze handeln, die miteinander verknüpft werden müssen. In unserem Falle handelt es sich um eine Datenquelle (eine `csv` Datei).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nutzen Sie diese Zelle in Google Colab\n",
    "#!wget -O \"BlackFriday.csv\" \"https://www.dropbox.com/s/92nc0agp631mpx3/BlackFriday.csv?dl=0\"\n",
    "#fpath = \"BlackFriday.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nutzen Sie diese Zelle in MyBinder\n",
    "fpath = \"./data/BlackFriday.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(fpath)\n",
    "df.head(10) # erste 10 Zeilen des Datensatzes ausgeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape # Datensatz hat 537.577 Zeilen und 12 Spalten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten bereinigen und für Analyse vorbereiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor die eigentliche Analyse der Daten beginnt - die natürlich abhängig ist von der spezifischen Fragestellung, die man hat - muss der Datensatz meist zunächst vorbereitet werden. \n",
    "\n",
    "Die Vorbereitung des Datensatzes umfasst einige wichtige Schritte, die abhängig sind von der Art des Datensatzes. Typischerweise beinhaltet die Vorbereitung\n",
    "\n",
    "- Umbenennung von Variablen\n",
    "\n",
    "\n",
    "- Bereinigung von Daten\n",
    "\n",
    "    - Datentypen identifizierne (kategorisch, numerisch etc.)\n",
    "    \n",
    "    - Identifikation von fehlenden Daten\n",
    "    \n",
    "    - Zusammenfügen von Datenquellen\n",
    "    \n",
    "    - Datensatz in das richtige Format bringen\n",
    "    \n",
    "Der vorbereitende Part der Datenanalyse umfasst oft 80% der gesamten Datenanalyse ([Dasu and Johnson (2003)](https://www.wiley.com/en-us/Exploratory+Data+Mining+and+Data+Cleaning-p-9780471268512)). Einen guter und oft zitierter Artikel zum Thema Vorbereitung der Daten finden Sie [hier](http://vita.had.co.nz/papers/tidy-data.pdf).\n",
    "\n",
    "Im weiteren Verlauf werden einige typische Schritte der Vorbereitung dargestellt. In Summe hat der vorliegende Datensatz jedoch bereits relativ gute Datenqualität. Das ist in der Realität oft nicht der Fall (und auch einer der Hauptgründe, warum Datenanalyse mit Excel oft sehr schwierig ist).\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Umbenennen von Variablen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oft ist es hilfreich die Namen von Variablen umzubenennen. Natürlich ist dies kein Muss, jedoch teilweise sehr hilfreich. \n",
    "\n",
    "- Variablen sollten selbsterklärend sein, da man ansonsten immer wieder nachschauen muss, was eine Variable inhaltlich aussagt  \n",
    "\n",
    "\n",
    "- Variablen sollten ohne Leerzeichen definiert werden (d.h. \"KäuferID\" und nicht \"Käufer ID\"). Auch das ist kein Muss, jedoch für die Implementierung mit Pandas oft hilfreich  \n",
    "\n",
    "\n",
    "- Variablen sollten dem Sprachgebrauch im Unternehmen entsprechen. Wenn das Unternehmen z.B. von Kunde spricht, ist es verwirrend, wenn die Variable AuftraggeberID heißt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns # Namen der Variablen im Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neue_header = [\"KäuferID\",\"ProduktID\",\"Geschlecht\",\"Alter\",\"Beruf\",\"StadtKategory\",\"JahreInStadt\",\"Verheiratet\",\n",
    "               \"ProduktKat1\",\"ProduktKat2\",\"ProduktKat3\",\"Umsatz\"]\n",
    "df.columns = neue_header\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bereinigung des Datensatzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variablentypen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für den Computer macht es einen Unterschied, ob es sich bei einer Variable um einen String oder um einen Float handelt. Insofern sollte man sich zunächst einen Überblick über die Datentypen machen. Oft sind durch das Einlesen numerische Wert in Strings umgewandelt etc. Ist dies der Fall muss man \"händisch\" eingreifen und die Typen der Variablen verändern. \n",
    "\n",
    "Auch sollte man analysieren, bei welchen Variablen es sich um Kategorien (hier z.b. Alter) handelt und bei welchen es sich um kontinuierliche Werte (hier z.B. Umsatz) handelt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() # objekt = String. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Variable \"Alter\" wird hier vom Typ \"objekt\" angegeben. Schauen wir uns an, warum das so ist und ob wir Alter nicht in eine Integer (Ganzzahl) umwandeln sollten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Alter\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sehen, dass \"Alter\" hier in Kategorien aufgeteilt wurde, es sich also nicht um das tatsächliche Alter des Kunden, sondern nur um eine Altersspanne handelt. Insofern macht es keinen Sinn (und ist auch nicht möglich) \"Alter\" in eine Integer umzuwandeln. \n",
    "\n",
    "Wieviel Kategorie für \"Alter\" gibt es denn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alters_kategorien = df[\"Alter\"].unique()\n",
    "len(alters_kategorien), alters_kategorien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt also sieben Alterskategorien. \n",
    "\n",
    "Pandas kann mit den Alterskategorien vom Typ \"String\" arbeiten. So kann z.B. der durchschnittliche Umsatz nach Alterskategorie wiefolgt ermittelt werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"Alter\"]).agg({\"Umsatz\":\"mean\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei großen Datensätzen ist es jedoch oft hilfreich kategorische Daten in den Typ \"category\" umzuwandeln, da Berechnungen dann oft deutlich schneller durchgeführt werden können. \n",
    "\n",
    "Das Umwandeln eines Datentypen erfolgt wiefolgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Alter\"] = df[\"Alter\"].astype(\"category\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finden wir doch heraus, bei welchen Variablen es sich noch um kategorische Variablen handelt und wandeln diese ebenfalls um. Alle Variablen mit wenigen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spalten = df.columns\n",
    "print(\"Variable\", \" | \", \"Anzahl an einzigartigen Werten\")\n",
    "print(\"===================================================\")\n",
    "for spalte in spalten:\n",
    "    uniques = df[spalte].unique()\n",
    "    print(spalte, \": \", len(uniques), \", Typ: \", df[spalte].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei den Variablen Geschlecht, Alter, Beruf, StadtKategory, JahreInStadt, Verheiratet sowie bei den Produktkategorien handelt es sich offensichtlich um kategorische Daten, da es zum einen nur eine begrenzte Zahl von einzigartigen Werten gibt und dies zum anderen auch inhaltlich Sinn macht. \n",
    "\n",
    "Hinweis: Bei z.B. \"JahreInStadt\" handelt es sich erst bei näherem Hinsehen um eine Kategorie, da der Wert \"4+\" nicht bestimmt ist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"JahreInStadt\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kategorische_spalten = [\"Geschlecht\",\"Alter\",\"Beruf\", \"StadtKategory\",\"JahreInStadt\",\"Verheiratet\",\"ProduktKat1\",\"ProduktKat2\",\"ProduktKat3\"]\n",
    "df[kategorische_spalten] = df[kategorische_spalten].astype(\"category\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch die Umwandlung der Variablen in \"category\" hat sich die Speichernutzung des Datensatzes von rund 45MB auf rund 17MB reduziert. Für diesen \"kleinen\" Datensatz war das nicht wirklich notwendig, jedoch ist es gut, sich von Anfang an Gedanken über die Art von Daten zu machen, mit denen man es zu tun hat. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifikation von fehlenden Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In den meisten Datensätzen in der Praxis wird es fehlende Datenpunkte geben. Sei es, weil der Kunde bestimmte Angaben nicht gemacht hat oder ein Vertriebler Informationen nicht abgefragt hat. Neben fehlenden Daten muss auch auf \"falsche\" Daten geachtet werden, der z.B. durch einen Typo entstanden ist. \n",
    "\n",
    "Fehlende Daten können auf verschiedene Arten identifiziert werden. \n",
    "\n",
    "Schaut man sich den vorliegenden Datensatz an, so sieht man, dass bereits in den ersten Zeilen in den Spalten \"ProduktKat2\" und ProduktKat3\" die Werte \"NaN\" auftauchen. \"NaN\" steht für \"Not a Number\" und ist ein Indikator für fehlende Daten. \n",
    "\n",
    "In der Praxis können aber auch einfach leere Zellen oder andere Identifikatoren auf fehlende Daten hinweisen. Oft merkt man erst im Verlaufe einer Analyse, dass bestimmte Daten fehlen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna() # gibt für jede Zelle True zurück, wenn NaN. Ansonsten False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Übersicht ist nur bedingt hilfreich. Wir sehen zwar, dass sich offensichtlich NaNs in zwei Spalten befinden, wissen aber nicht, ob sich nicht auch in anderen Spalten irgendwo (z.B. in Zeile 312.456...) verstecken. \n",
    "\n",
    "Insofern ist es hilfreich die Summe der NaNs je Spalte zu bestimmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() # bestimmt, wieviel NaNs in den jeweiligen Spalten enthalten sind. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glück gehabt. In unserem Datensatz gibt es nur NaN in zwei der 12 Spalten. \n",
    "\n",
    "Im nächsten Schritt müssen wir entscheiden, wie wir damit umgehen. Dies hängt natürlich von der Fragestellung der Analyse ab. \n",
    "\n",
    "Lautet die Fragestellung z.B.: was ist der durchschnittliche Umsatz je ProduktKat3 nach Geschlecht, können wir die Zeilen mit fehlenden Werten für ProduktKat3 offensichtlich löschen (bzw. sicherstellen, dass es sich tatsächlich um \"NaN\" handelt, mit denen Pandas umgehen kann). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.dropna(subset=[\"ProduktKat3\"]) \n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Datensatz ist jedoch jetzt deutlich kleiner. Ich sollte also keine Zeilen löschen, wenn sich die Fragestellung eigentlichc auf zB. ProduktKat1 bezieht, da ich ansonsten über 300.000 Zeilen an Daten unberücksichtigt lasse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Berichtigen von Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oft kann es auch vorkommen, dass Daten verändert, korrigiert oder berichtigt werden müssen.\n",
    "\n",
    "Stellen wir uns in unserem Fall z.B. vor, dass die ProduktIDs falsch sein. Diese fangen alle mit einem \"P\" an. Es könnte z.B. sein, dass wir die Daten später weiterverarbeiten müssen und mit einem anderen Datensatz zusammenführen wollen. In diesem Datensatz beginnen die ProduktIDs alle mit \"Z\", obwohl es sich um die selben Produkte handelt (ein zugegeben etwas unsinniges Beispiel). \n",
    "\n",
    "In Realität kann sowas aber durchaus vorkommen, wenn Daten aus verschiedenen Datenquellen (Systemen) zusammgeführt werden müssen und dort Kunden leicht abweichende Namen haben oder es sich um Systeme aus unterschiedlichen Unternehmensteilen handelt etc. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ProduktID\"] = df[\"ProduktID\"].str.replace(\"P\",\"Z\",1) # ersetzt den ersten Buchstaben \"P\" durch ein \"Z\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fazit**\n",
    "\n",
    "Es ist deutlich geworden, dass mithilfe von Python und Pandas die Bereinigung von Daten sehr einfach und mit oft jeweils nur kurzen Befehlen möglich ist. \n",
    "\n",
    "Natürlich handelt es sich hier nur um eine sehr kurze Einführung in die wichtigen Schritte beim Vorbereiten eines Datensatz. Die tatsächlich notwendigen Schritte ergeben sich erst aus dem konkreten Datensatz und sind auch teilweise nicht von Anfang an ersichtlich, so dass es sich hier meist um einen iterativen Prozess handelt, bei dem man sich bereits im nächsten Schritt - der eigentliichen Datenanalyse - wähnt und dann aber doch feststellen muss, dass es noch Fehler in den Daten gibt oder weitere Daten hinzukommen etc. \n",
    "\n",
    "Gerade hier spielt die Einbettung von Python in ein Jupyter Notebook die großen Vorteile aus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenanalyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem man sich im vorangegangenen Schritt einen Überblick zum Datensatz gemacht hat, kann man mit der eigentlichen Datenanalyse beschäftigen. \n",
    "\n",
    "Diese hängt natürlich sehr von der spezifischen Fragestellung ab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel: konkrete deskriptive Fragestellung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ist diese sehr konkret, z.B. was ist der durchschnittliche Umsatz je Berufsgruppe nach Geschlecht, kann man diese spezifische Fragestellung analysieren.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"Geschlecht\"]).agg({\"Umsatz\":\"sum\"}).plot(kind=\"bar\", title=\"Gesamtumsatz nach Geschlecht\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"Geschlecht\"]).agg({\"Umsatz\":\"mean\"}).plot(kind=\"bar\", title=\"Durchschnittlicher Umsatz nach Geschlecht\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"Beruf\"]).agg({\"Umsatz\":\"mean\"}).plot(kind=\"bar\", title=\"Durchschnittlicher Umsatz nach Beruf\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es scheint keine signifikanten Unterschiede beim Umsatz nach Geschlecht und Beruf zu geben.\n",
    "\n",
    "Im nächsten Schritt könnte man versuchen, die Ergebnisse zusammenzufassen und die Visualisierung etwas \"aufzuhübschen\". \n",
    "\n",
    "**Beispiel zur \"Aufhübschung\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2,nrows=2, sharey=True,figsize=(12,9))\n",
    "axes = axes.flatten()\n",
    "sns.despine()\n",
    "\n",
    "# Erster Chart\n",
    "x = [0,1]\n",
    "y = df.groupby([\"Geschlecht\"]).agg({\"Umsatz\":\"sum\"})[\"Umsatz\"] / 1000000\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.bar(x,y)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([\"Frau\",\"Mann\"])\n",
    "ax1.set_title(\"Gesamtumsatz nach Geschlecht \\n(in mio. EUR)\");\n",
    "\n",
    "# Zweiter Chart\n",
    "x = [0,1]\n",
    "y = df.groupby([\"Geschlecht\"]).agg({\"Umsatz\":\"mean\"})[\"Umsatz\"]\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.bar(x,y)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([\"Frau\",\"Mann\"])\n",
    "ax2.set_title(\"Gesamtumsatz nach Geschlecht \\n (in EUR)\");\n",
    "\n",
    "# Dritter Chart\n",
    "x = range(len(df[\"Beruf\"].unique()))\n",
    "y = df.groupby([\"Beruf\"]).agg({\"Umsatz\":\"mean\"})[\"Umsatz\"]\n",
    "\n",
    "ax3 = axes[2]\n",
    "ax3.bar(x,y)\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(x)\n",
    "ax3.set_title(\"Durchschnittlicher Umsatz nach Beruf \\n (in EUR)\");\n",
    "\n",
    "\n",
    "# Vierter Chart\n",
    "ax4 = axes[3]\n",
    "df.groupby([\"Geschlecht\",\"Beruf\"]).agg({\"Umsatz\":\"mean\"}).unstack().T.plot.bar(stacked=True,\n",
    "    title=\"Durschnittlicher Umsatz nach Geschlecht und Beruf \\n(in EUR)\", ax=ax4);\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(x,rotation=\"horizontal\")\n",
    "\n",
    "plt.suptitle(\"Umsatzanalyse nach Geschlecht und Beruf\",fontsize=21,y=1.05)\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"Umsatzanalyse.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gäbe natürlich noch sehr viel weitere Möglichkeiten, die Analyse durchzuführen und die Darstellungen ansprechender zu machen. Dies soll an dieser Stelle nur ein Beispiel sein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel: explorative Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Realität haben Unternehmen oft Unmengen an Daten und haben neben sehr konkreten Fragestellungen zusätzlich Interesse daran, neue Muster zu erkennen und diese für ökonomische Entscheidungen zu nutzen. \n",
    "\n",
    "Vielleicht fällt bei genauerer Betrachtung auf, dass Kunden, die bereits lange in der Stadt leben öfter Produkte mit hohen Gewinnmargen kaufen. Dies wäre ein Zusammenhang, den man vermutlich nicht konkret erfragen würde, sondern den man eher zufällig aus den Daten herausfiltern würde. Die neu gewonnen Information könnte man dann z.B. nutzen, um noch gezielter diese Art von Kunden anzusprechen etc. \n",
    "\n",
    "Diese Art von Informationen wird also gewonnen, in dem ein Datensatz (i) um so viel wie mögliche neue Variablen ergänzt wird (z.b. Wetter, Temperatur, Feiertag....) und (ii) frei analysiert wird, d.h. viele Kombinationen eruiert werden etc. \n",
    "\n",
    "Es wird also tatsächlich explorativ vorgegangen und systematisch nach neuen Zusammenhängen gesucht (die oft dann aber auch nicht gefunden werden). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spalten = df.columns[2:-1] # alle Spalten außer KundenID, ProduktID und Umsatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for spalte in spalten:\n",
    "    df.groupby(spalte).agg({\"Umsatz\":\"mean\"}).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gbit unzählige Möglichkeiten von hier weiter vorzugehen. Insbsondere kann man versuchen statistische Modelle aufzustellen, die Zusammenhänge überprüfen. Auch hier gibt es Bibliotheken (z.B. [`scikit`](https://scikit-learn.org/stable/)), die die hierfür notwendigen Schritte mit vielen Funktionalitäten unterstützen. Zunächst müssten bspw. kategorische Variablen so umgewandelt werden, dass mit diesen gerechnet werden kann. Die dann anschließende statistiche Analyse wird dann ebenfalls durch Modelle unterstützt.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wichtig:** Im Rahmen dieses Seminars wird der Fokus auf der Einführung in die Datenanalyse gelegt, d.h. es sollen\n",
    "\n",
    "- Grundlagen in Python (inkl. Numpy, Pandas und Matplotlib) erarbeitet werden  \n",
    "\n",
    "\n",
    "- ein Datensatz mit Pandas deskriptiv vorbereitet, analysiert und vorgestellt werden\n",
    "\n",
    "Es wird nicht erwartet, dass statistische Analyse durchgeführt werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<center><< zurück zu Module nutzen</center>](04_Module_nutzen.ipynb)[<center>weiter zur Aufgabenstellung >></center>](06_Aufgabenstellung.ipynb)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.991px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
